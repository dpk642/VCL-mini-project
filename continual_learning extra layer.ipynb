{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Softmax\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mnist.pkl\",'rb') as f:\n",
    "    train_set, valid_set, test_set = pkl.load(f,encoding='bytes')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(number):   #filters through the training/test data to find ones with specified label\n",
    "    new_train = []\n",
    "    new_test = []\n",
    "    for i,label in enumerate(train_set[1]): #find data with label \"number\" in training set\n",
    "        if label == number:\n",
    "            new_train.append(train_set[0][i])\n",
    "    for i,label in enumerate(valid_set[1]): #find data with label \"number\" in validation set and add to training set (since no need for validation in VCL)\n",
    "        if label == number:\n",
    "            new_train.append(train_set[0][i])\n",
    "    for i,label in enumerate(test_set[1]):#find data with label \"number\" in test set for testing\n",
    "        if label == number:\n",
    "            new_test.append(train_set[0][i])\n",
    "    return new_train,new_test\n",
    "\n",
    "def generate_task_data(tasknumber):\n",
    "    data_0 = get_data(tasknumber)\n",
    "    data_1 = get_data(tasknumber + 1)  \n",
    "             \n",
    "    trainx = np.vstack((data_0[0],data_1[0]))\n",
    "    zeros =  np.zeros((len(data_1[0]), 1))\n",
    "    ones =  np.ones((len(data_0[0]), 1))\n",
    "    trainy = np.vstack((ones,zeros))# create labels for classes\n",
    "    trainy = np.hstack((trainy,1-trainy))# format for cross entropy loss (-1*likelihood) downstream\n",
    "\n",
    "    testx = np.vstack((data_0[1],data_1[1]))\n",
    "    zeros =  np.zeros((len(data_1[1]), 1))\n",
    "    ones =  np.ones((len(data_0[1]), 1))\n",
    "    testy = np.vstack((ones,zeros))\n",
    "    testy = np.hstack((testy,1-testy))\n",
    "    return trainx,trainy,testx,testy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, loss: 274.4540\n",
      "Epoch: 3/100, loss: 188.4021\n",
      "Epoch: 5/100, loss: 181.4164\n",
      "Epoch: 7/100, loss: 170.6639\n",
      "Epoch: 9/100, loss: 158.9686\n",
      "Epoch: 11/100, loss: 146.1150\n",
      "Epoch: 13/100, loss: 134.3355\n",
      "Epoch: 15/100, loss: 121.9030\n",
      "Epoch: 17/100, loss: 108.8356\n",
      "Epoch: 19/100, loss: 99.3294\n",
      "Epoch: 21/100, loss: 96.7782\n",
      "Epoch: 23/100, loss: 87.8035\n",
      "Epoch: 25/100, loss: 81.4479\n",
      "Epoch: 27/100, loss: 75.4425\n",
      "Epoch: 29/100, loss: 68.6143\n",
      "Epoch: 31/100, loss: 69.5818\n",
      "Epoch: 33/100, loss: 68.5809\n",
      "Epoch: 35/100, loss: 65.3790\n",
      "Epoch: 37/100, loss: 71.1018\n",
      "Epoch: 39/100, loss: 59.3254\n",
      "Epoch: 41/100, loss: 53.1488\n",
      "Epoch: 43/100, loss: 53.5741\n",
      "Epoch: 45/100, loss: 55.4159\n",
      "Epoch: 47/100, loss: 53.6616\n",
      "Epoch: 49/100, loss: 54.4257\n",
      "Epoch: 51/100, loss: 49.5054\n",
      "Epoch: 53/100, loss: 56.0544\n",
      "Epoch: 55/100, loss: 50.9661\n",
      "Epoch: 57/100, loss: 45.0319\n",
      "Epoch: 59/100, loss: 42.3179\n",
      "Epoch: 61/100, loss: 43.3004\n",
      "Epoch: 63/100, loss: 43.6162\n",
      "Epoch: 65/100, loss: 51.3333\n",
      "Epoch: 67/100, loss: 53.9323\n",
      "Epoch: 69/100, loss: 40.0451\n",
      "Epoch: 71/100, loss: 38.4381\n",
      "Epoch: 73/100, loss: 38.7246\n",
      "Epoch: 75/100, loss: 40.5330\n",
      "Epoch: 77/100, loss: 52.3026\n",
      "Epoch: 79/100, loss: 47.1018\n",
      "Epoch: 81/100, loss: 40.1430\n",
      "Epoch: 83/100, loss: 36.6109\n",
      "Epoch: 85/100, loss: 38.9129\n",
      "Epoch: 87/100, loss: 35.9164\n",
      "Epoch: 89/100, loss: 35.5507\n",
      "Epoch: 91/100, loss: 37.6230\n",
      "Epoch: 93/100, loss: 36.0677\n",
      "Epoch: 95/100, loss: 35.4817\n",
      "Epoch: 97/100, loss: 39.8674\n",
      "Epoch: 99/100, loss: 47.2348\n",
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "1093\n",
      "/\n",
      "2115\n"
     ]
    }
   ],
   "source": [
    "#First train a regular NN to find appropriate initialisation values for bayesian network\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, n_inputs: int):\n",
    "    super(MLP, self).__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "        nn.Linear(n_inputs,256),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256,256),\n",
    "        #nn.Softmax(),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(256,2),\n",
    "        \n",
    "        #nn.LogSoftmax()\n",
    "\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "def ce_loss(logits,targets):\n",
    "  logprobs = torch.nn.functional.log_softmax(logits)\n",
    "  return -(targets*logprobs).sum()\n",
    "# ###checking###\n",
    "def train_model(x, y, model, lr: float=0.005, n_epochs: int=100):\n",
    "  bs = 500\n",
    "  optimiser = torch.optim.Adam(model.parameters(), lr=lr)#,weight_decay=5)\n",
    "  dloader = DataLoader(list(zip(x, y)), shuffle=True, batch_size=bs)\n",
    "  for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    for batch_x, batch_y in dloader:\n",
    "      optimiser.zero_grad()\n",
    "      output = model(batch_x)\n",
    "      loss_val = torch.binary_cross_entropy_with_logits(output,batch_y).sum()\n",
    "      loss_val.backward()\n",
    "      optimiser.step()\n",
    "\n",
    "      train_loss += loss_val.item() * len(batch_x)\n",
    "\n",
    "    train_loss /= len(x)\n",
    "\n",
    "    if epoch % 2  == 0:\n",
    "      print(f'Epoch: {epoch+1}/{n_epochs}, loss: {train_loss:.4f}')\n",
    "\n",
    "  return model\n",
    "data = generate_task_data(0)\n",
    "model = MLP(n_inputs=784)\n",
    "initial_model = train_model(torch.tensor(data[0],dtype=torch.float32),torch.tensor(data[1],dtype=torch.float32),model)\n",
    "print(initial_model)\n",
    "ans = torch.tensor(data[3].argmax(axis=1))\n",
    "succ = 0\n",
    "preds = torch.softmax(initial_model(torch.tensor(data[2])),dim=1).argmax(dim=1)\n",
    "\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == ans[i]:\n",
    "        succ +=1\n",
    "print(succ)\n",
    "print(\"/\")\n",
    "print(len(ans))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates first prior:\n",
    "task_weights_means = []   #To hold task (n) specific head means in position (n)\n",
    "task_weights_variances = []  #for each entry : value i,j is the variance that would be the diagonal entry on the cov matrix for the associated mean field approximation\n",
    "task_bias_means = []\n",
    "task_bias_variances = []\n",
    "\n",
    "shared_weights_mean = []\n",
    "shared_weights_variances = []\n",
    "shared_bias_mean = []\n",
    "shared_bias_variances = []\n",
    "\n",
    "#first shared layer weights and biases\n",
    "shared_weights_mean.append(initial_model.layers[0].weight.clone().detach())#Extract shared means from regular neural network for initialisation of shared hidden layers\n",
    "shared_bias_mean.append(initial_model.layers[0].bias.clone().detach())  #Also use .clone.detach to instantiate it as tensor for autograd downstream and remove from previous computation graph\n",
    "shared_weights_variances.append(torch.full(shared_weights_mean[0].shape,-6,requires_grad=False,dtype=torch.float32))  #initialise variances as 10^-6 as per paper\n",
    "shared_bias_variances.append(torch.full(shared_bias_mean[0].shape,-6,requires_grad=False,dtype=torch.float32))        #we are storing variances as log variances to prevent negative variance\n",
    "\n",
    "#Second layer shared weights and biases\n",
    "shared_weights_mean.append(initial_model.layers[2].weight.clone().detach())\n",
    "shared_bias_mean.append(initial_model.layers[2].bias.clone().detach())\n",
    "shared_weights_variances.append(torch.full(shared_weights_mean[1].shape,-6,requires_grad=False,dtype=torch.float32))\n",
    "shared_bias_variances.append(torch.full(shared_bias_mean[1].shape,-6,requires_grad=False,dtype=torch.float32))\n",
    "\n",
    "#initialise first task head weights/variances\n",
    "task_weights_means.append((initial_model.layers[4].weight).clone().detach()) #Extract task1 specific means for initialisation of first head task\n",
    "task_bias_means.append((initial_model.layers[4].bias).clone().detach())\n",
    "\n",
    "#task1_initial_weight_variance = torch.full_like(initial_model.layers[4].weight,-6,requires_grad=False,dtype=torch.float32)#initialise variances as 10^-6 as per paper\n",
    "#task1_initial_bias_variance = torch.full_like(initial_model.layers[4].bias,-6,requires_grad=False,dtype=torch.float32)\n",
    "\n",
    "task_weights_variances.append(torch.full_like(initial_model.layers[4].weight,-6,requires_grad=False,dtype=torch.float32))# store task specific variances \n",
    "task_bias_variances.append(torch.full_like(initial_model.layers[4].bias,-6,requires_grad=False,dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1weightmeans,l1weightvariances,l1biasmeans,l1biasvariances,l2weightmeans,l2weightvariances,l2biasmeans,l2biasvariances = [],[],[],[],[],[],[],[] #bayesian model params\n",
    "opt_params = []\n",
    "for i in range(len(shared_weights_mean)):\n",
    "    l1weightmeans.append(shared_weights_mean[i].detach().clone())   #copy weights and biases from mlp to be initialisation for means in bayesian model\n",
    "    l1weightvariances.append(shared_weights_variances[i].detach().clone())\n",
    "    l1biasmeans.append(shared_bias_mean[i].detach().clone())\n",
    "    l1biasvariances.append(shared_bias_variances[i].detach().clone())\n",
    "    l1weightmeans[i].requires_grad = True   #Set requires grad to True on bayesian model params to be updated in autodiff\n",
    "    l1weightvariances[i].requires_grad = True\n",
    "    l1biasmeans[i].requires_grad = True\n",
    "    l1biasvariances[i].requires_grad = True\n",
    "    opt_params.append(l1weightmeans[i])\n",
    "    opt_params.append(l1weightvariances[i])\n",
    "    opt_params.append(l1biasmeans[i])\n",
    "    opt_params.append(l1biasvariances[i])\n",
    "    \n",
    "\n",
    "for i in range(len(task_weights_means)):   #Ensure models parameters will be updated through autograd for every task specific parameter\n",
    "    l2weightmeans.append(task_weights_means[i].detach().clone())\n",
    "    l2weightvariances.append(task_weights_variances[i].detach().clone())\n",
    "    l2biasmeans.append(task_bias_means[i].detach().clone())\n",
    "    l2biasvariances.append(task_bias_variances[i].detach().clone())\n",
    "    l2weightmeans[i].requires_grad = True\n",
    "    l2weightvariances[i].requires_grad = True\n",
    "    l2biasmeans[i].requires_grad = True\n",
    "    l2biasvariances[i].requires_grad = True\n",
    "    opt_params.append(l2weightmeans[i])\n",
    "    opt_params.append(l2weightvariances[i])\n",
    "    opt_params.append(l2biasmeans[i])\n",
    "    opt_params.append(l2biasvariances[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def KL_divergence():#l1weightmeans,l1weightvariances,l1biasmeans,l1biasvariances,l2weightmeans,l2weightvariances,l2biasmeans,l2biasvariances,shared_weights_mean,shared_weights_variances,shared_bias_mean,shared_bias_variances,task_weights_means,task_weights_variances,task_bias_means,task_bias_variances):\n",
    "def KL_divergence(l1weightmeans,l1weightvariances,l1biasmeans,l1biasvariances,l2weightmeans,l2weightvariances,l2biasmeans,l2biasvariances,shared_weights_mean,shared_weights_variances,shared_bias_mean,shared_bias_variances,task_weights_means,task_weights_variances,task_bias_means,task_bias_variances):\n",
    "    KL = 0 #ignore constant term as it does not contribute to optimisation process\n",
    "    mu_term = 0\n",
    "    variance_term = 0\n",
    "    log_var_term = 0\n",
    "\n",
    "    for i in range(len(shared_weights_mean)):\n",
    "        mu_term += (1/2) * torch.sum(torch.mul(torch.square(shared_weights_mean[i]-l1weightmeans[i]),(torch.pow(torch.exp(shared_weights_variances[i]),-2))))  #mu term for first layer weights\n",
    "        variance_term += (1/2) * torch.sum(torch.mul(torch.pow(torch.exp(shared_weights_variances[i]),-2),torch.square(torch.exp(l1weightvariances[i]))))  \n",
    "        log_var_term += torch.sum(torch.subtract(shared_weights_variances[i],l1weightvariances[i])) # only need to subtract and ignore prefactor since we are dealing with log variances to start with\n",
    "        #print(mu_term,variance_term,log_var_term)                                                  # since 0.5 * log(a^2/b^2) =(log a - log b )\n",
    "        mu_term += (1/2) * torch.sum(torch.mul(torch.square(shared_bias_mean[i]-l1biasmeans[i]),(torch.pow(torch.exp(shared_bias_variances[i]),-2)))) # add contributions from first layer biases \n",
    "        variance_term += (1/2) * torch.sum(torch.mul(torch.square(torch.exp(l1biasvariances[i])),torch.pow(torch.exp(shared_bias_variances[i]),-2)))\n",
    "        log_var_term += torch.sum(torch.subtract(shared_bias_variances[i],l1biasvariances[i])) \n",
    "\n",
    "    for task in range(len(l2weightmeans)): # contribution for each task weights \n",
    "        mu_term += 1/2 * torch.sum(torch.mul(torch.square(task_weights_means[task] - l2weightmeans[task]),(torch.pow(torch.exp(task_weights_variances[task]),-2))))\n",
    "        variance_term += 1/2 * torch.sum(torch.mul(torch.pow(torch.exp(task_weights_variances[task]),-2),torch.square(torch.exp(l2weightvariances[task]))))\n",
    "        log_var_term += torch.sum(torch.subtract(task_weights_variances[task],l2weightvariances[task]))     #torch.sum(torch.log(torch.square(torch.div(task_weights_variances[task][i],l2weightvariances[task][i]))))\n",
    "\n",
    "      #contribution from task biases\n",
    "    for task in range(len(l2biasmeans)):#for each task  \n",
    "        #for i in range(len(task_bias_means[task])):     \n",
    "        mu_term += 1/2 * torch.sum(torch.mul(torch.square(task_bias_means[task]-l2biasmeans[task]),(torch.pow(torch.exp(task_bias_variances[task]),-2))))\n",
    "        variance_term += 1/2 * torch.sum(torch.mul(torch.pow(torch.exp(task_bias_variances[task]),-2),torch.square(torch.exp(l2biasvariances[task]))))\n",
    "        log_var_term += torch.sum(torch.subtract(task_bias_variances[task],l2biasvariances[task]))   # torch.sum(torch.log(torch.square(torch.div(task_bias_variances[task],l2biasvariances[task]))))\n",
    "    KL = mu_term + variance_term + log_var_term\n",
    "    print(f\"calculated KL divergence {KL - 133633}\") #include constant term for completeness\n",
    "    return (KL - 133633)\n",
    "def cross_entropy(inputs,targets):\n",
    "    result = torch.mul(torch.log(inputs),targets)\n",
    "    return -1*torch.sum(result) #since cross entropy is negative log likelihood for binary classification\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bayesian_network():#l1weightmeans,l1weightvariances,l1biasmeans,l1biasvariances,l2weightmeans,l2weightvariances,l2biasmeans,l2biasvariances):\n",
    "\n",
    "    #def forward(self,input,iterations,task,l1weightmeans,l1weightvariances,l1biasmeans,l1biasvariances,l2weightmeans,l2weightvariances,l2biasmeans,l2biasvariances):\n",
    "    def forward(self,input,iterations,task,*args):\n",
    "                                                  #number of samples for MC integration of likelihood is iterations\n",
    "      print(f\"task is {task}\")\n",
    "      l2wm,l2wv,l2bm,l2bv = l2weightmeans[task].tile([iterations,1,1]),l2weightvariances[task].tile([iterations,1,1]),l2biasmeans[task].tile([iterations,1]),l2biasvariances[task].tile([iterations,1])\n",
    "      l2bv,l2bm = l2bv.unsqueeze(-1).expand(-1,-1,len(input.transpose(0,1))),l2bm.unsqueeze(-1).expand(-1,-1,len(input.transpose(0,1))) #tile to compute all normal samples simultaneousluy\n",
    "      for i in range(len(l1weightmeans)):\n",
    "        l1wm,l1wv,l1bm,l1bv = l1weightmeans[i].tile([iterations,1,1]),l1weightvariances[i].tile([iterations,1,1]),l1biasmeans[i].tile(([iterations,1])),l1biasvariances[i].tile([iterations,1])\n",
    "        l1bv,l1bm = l1bv.unsqueeze(-1).expand(-1,-1,len(input.transpose(0,1))),l1bm.unsqueeze(-1).expand(-1,-1,len(input.transpose(0,1)))\n",
    "        weightnormal = torch.normal(torch.zeros_like(l1wm),torch.ones_like(l1wv))#sample N(0,1) in order to realise weights/biases for layer 1\n",
    "        biasnormal = torch.normal(torch.zeros_like(l1bm),torch.ones_like(l1bv))\n",
    "        if i == 0:\n",
    "          result = torch.add(l1wm,torch.mul(weightnormal,torch.exp(l1wv)))\n",
    "          result = torch.add(torch.matmul(result,input),torch.add(l1bm,torch.mul(torch.exp(l1bv),biasnormal)))\n",
    "        if i == 1 :\n",
    "          result = torch.matmul(torch.add(l1wm,torch.mul(l1wv,weightnormal)),result)\n",
    "        result = torch.relu(result)\n",
    "        \n",
    "      ###forward pass for task specific head \n",
    "      weightnormal = torch.normal(torch.zeros_like(l2wm),torch.ones_like(l2wv))#sample N(0,1) in order to realise weights/biases for layer 2\n",
    "      biasnormal = torch.normal(torch.zeros_like(l2bm),torch.ones_like(l2bv))\n",
    "      result = torch.matmul(torch.add(l2wm,torch.mul(torch.exp(l2wv),weightnormal)),result)\n",
    "      result = torch.squeeze(result,-1)\n",
    "      result = torch.add(result,torch.add(l2bm,torch.mul(torch.exp(l2bv),biasnormal)))\n",
    "      result = torch.mean(result,dim=0)\n",
    "\n",
    "      return result.transpose(0,1)\n",
    "  \n",
    "    def loss(self,prediction,label):#,label,l1weightmeans,l1weightvariances,l1biasmeans,l1biasvariances,l2weightmeans,l2weightvariances,l2biasmeans,l2biasvariances,shared_weights_mean,shared_weights_variances,shared_bias_mean,shared_bias_variances,task_weights_means,task_weights_variances,task_bias_means,task_bias_variances):   # we will minimise loss so we want a loss that is KL - log_lik = KL + cross entropy\n",
    "      loss = torch.tensor(0,dtype=torch.float32)#,requires_grad=True)\n",
    "      loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "      loss = loss + loss_fn(prediction,label)\n",
    "      #print(f\"KL and likelihood are {KL_divergence().item(),}\")\n",
    "      #loss += torch.binary_cross_entropy_with_logits(prediction,label)\n",
    "      loss = loss + KL_divergence(l1weightmeans,l1weightvariances,l1biasmeans,l1biasvariances,l2weightmeans,l2weightvariances,l2biasmeans,l2biasvariances,shared_weights_mean,shared_weights_variances,shared_bias_mean,shared_bias_variances,task_weights_means,task_weights_variances,task_bias_means,task_bias_variances)\n",
    "      #l1weightmeans,l1weightvariances,l1biasmeans,l1biasvariances,l2weightmeans,l2weightvariances,l2biasmeans,l2biasvariances,shared_weights_mean,shared_weights_variances,shared_bias_mean,shared_bias_variances,task_weights_means,task_weights_variances,task_bias_means,task_bias_variances)\n",
    "      \n",
    "      return loss\n",
    "#def train_bayes_model(self,x, y, lr: float=1e-3, n_epochs: int=120):\n",
    "    def train_bayes_model(self,x, y,task, lr: float=1e-3, n_epochs: int=50):\n",
    "    \n",
    "      optimiser = torch.optim.Adam(params=[*l1weightmeans,*l1weightvariances,*l1biasmeans,*l1biasvariances,*l2weightmeans,*l2weightvariances,*l2biasmeans,*l2biasvariances],lr= lr)#[l1weightmeans[0],l1weightmeans[1],l1weightvariances[0],l1weightvariances[1],l1biasmeans[0],l1biasmeans[1],l1biasvariances[0],l1biasvariances[1],l2weightmeans[task],l2weightvariances[task],l2biasmeans[task],l2biasvariances[task]], lr=lr)\n",
    "      dloader = DataLoader(list(zip(x, y)), shuffle=True, batch_size=len(x))\n",
    "      train_loss = torch.tensor(0,dtype=torch.float32)\n",
    "      for epoch in range(n_epochs):       \n",
    "        epoch_loss = torch.tensor(0,dtype=torch.float32)\n",
    "        optimiser.zero_grad()\n",
    "        for batch_x, batch_y in dloader:\n",
    "          epoch_loss = epoch_loss +self.loss(self.forward(batch_x.transpose(0,1),10,task,l1weightmeans,l1weightvariances,l1biasmeans,l1biasvariances,l2weightmeans,l2weightvariances,l2biasmeans,l2biasvariances),batch_y)\n",
    "        print(f\"epoch loss is {epoch_loss.item()}\")\n",
    "        epoch_loss.backward()\n",
    "        optimiser.step()\n",
    "        train_loss += epoch_loss\n",
    "        print(f\"completed epoch {epoch}, stepping optimiser\")  \n",
    "        if epoch % 20  == 0:\n",
    "          print(f'Epoch: {epoch+1}/{n_epochs}, loss: {train_loss:.4f}')\n",
    "\n",
    "      return [l1weightmeans,l1weightvariances,l1biasmeans,l1biasvariances,l2weightmeans,l2weightvariances,l2biasmeans,l2biasvariances]\n",
    "\n",
    "\n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task is 0\n",
      "calculated KL divergence 0.0\n",
      "epoch loss is 10.831725030486075\n",
      "completed epoch 0, stepping optimiser\n",
      "Epoch: 1/10, loss: 10.8317\n",
      "task is 0\n",
      "calculated KL divergence 7259.46875\n",
      "epoch loss is 7266.2676557095665\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[221], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m bn \u001b[38;5;241m=\u001b[39m bayesian_network()\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m generate_task_data(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m trained_params \u001b[38;5;241m=\u001b[39m \u001b[43mbn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_bayes_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[219], line 59\u001b[0m, in \u001b[0;36mbayesian_network.train_bayes_model\u001b[1;34m(self, x, y, task, lr, n_epochs)\u001b[0m\n\u001b[0;32m     57\u001b[0m   epoch_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch_x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m),\u001b[38;5;241m10\u001b[39m,task,l1weightmeans,l1weightvariances,l1biasmeans,l1biasvariances,l2weightmeans,l2weightvariances,l2biasmeans,l2biasvariances),batch_y)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch loss is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m \u001b[43mepoch_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     61\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m epoch_loss\n",
      "File \u001b[1;32mc:\\Users\\deepa\\anaconda3\\envs\\Deep_learning\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\deepa\\anaconda3\\envs\\Deep_learning\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bn = bayesian_network()\n",
    "data = generate_task_data(0)\n",
    "trained_params = bn.train_bayes_model(data[0], data[1],task = 0 , lr = float(0.001),n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_prior_from_posterior():  #updates prior with current posterior ready to build next bead\n",
    "    for i in range(len(l1weightmeans)):\n",
    "        shared_weights_mean[i] = l1weightmeans[i].clone().detach()   #change prior to current models params but detach so they retian requires_grad = False\n",
    "        shared_weights_variances[i] = l1weightmeans[i].clone().detach()\n",
    "        shared_bias_mean[i] = l1biasmeans[i].clone().detach()\n",
    "        shared_bias_variances[i] = l1biasvariances[i].clone().detach()\n",
    "        \n",
    "    for i in range(len(task_weights_means)):   #Ensure models parameters will be updated through autograd for every task specific parameter\n",
    "        task_weights_means[i] = l2weightmeans[i].clone().detach()\n",
    "        task_weights_variances[i] = l2weightvariances[i].clone().detach()\n",
    "        task_bias_means[i] = l2biasmeans[i].clone().detach()\n",
    "        task_bias_variances[i] = l2biasvariances[i].clone().detach()\n",
    "    return None\n",
    "def add_new_task_params(): #add model paramaters for the next task head\n",
    "    l2weightmeans.append(torch.normal(mean = torch.zeros(l2weightmeans[0].shape))) #mean 0 std 1 with same shape as other output layers\n",
    "    l2weightvariances.append(torch.full(l2weightvariances[0].shape,-6,dtype=torch.float32))  # variances initialsied 10^-6 as in paper\n",
    "    l2biasmeans.append(torch.normal(mean = torch.zeros(l2biasmeans[0].shape)))\n",
    "    l2biasvariances.append(torch.full(l2biasvariances[0].shape,-6,dtype=torch.float32))\n",
    "    \n",
    "    task_weights_means.append(l2weightmeans[-1].detach())#copy model parameters into prior and detach since they are the prior \n",
    "    task_weights_variances.append(l2weightvariances[-1].detach())\n",
    "    task_bias_means.append(l2biasmeans[-1].detach())\n",
    "    task_bias_variances.append(l2biasvariances[-1].detach())\n",
    "    \n",
    "\n",
    "    l2weightmeans[-1].requires_grad = True\n",
    "    l2weightvariances[-1].requires_grad = True\n",
    "    l2biasmeans[-1].requires_grad = True\n",
    "    l2biasvariances[-1].requires_grad = True\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task is 0\n",
      "calculated KL divergence 1458238.375\n",
      "epoch loss is 1458239.5184938477\n",
      "completed epoch 0, stepping optimiser\n",
      "Epoch: 1/300, loss: 1458239.5000\n",
      "task is 0\n",
      "calculated KL divergence 1458211.125\n",
      "epoch loss is 1458211.652197572\n",
      "completed epoch 1, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1458183.25\n",
      "epoch loss is 1458184.2878089326\n",
      "completed epoch 2, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1458155.875\n",
      "epoch loss is 1458156.802481062\n",
      "completed epoch 3, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1458129.125\n",
      "epoch loss is 1458129.7479860482\n",
      "completed epoch 4, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1458102.375\n",
      "epoch loss is 1458103.2561366821\n",
      "completed epoch 5, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1458075.375\n",
      "epoch loss is 1458076.1461678136\n",
      "completed epoch 6, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1458048.75\n",
      "epoch loss is 1458050.268219961\n",
      "completed epoch 7, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1458022.25\n",
      "epoch loss is 1458023.4605234582\n",
      "completed epoch 8, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457995.75\n",
      "epoch loss is 1457996.979047949\n",
      "completed epoch 9, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457969.25\n",
      "epoch loss is 1457970.0136717542\n",
      "completed epoch 10, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457942.25\n",
      "epoch loss is 1457942.9713867945\n",
      "completed epoch 11, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457915.625\n",
      "epoch loss is 1457916.4191866969\n",
      "completed epoch 12, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457888.875\n",
      "epoch loss is 1457890.073702797\n",
      "completed epoch 13, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457862.125\n",
      "epoch loss is 1457863.0651673041\n",
      "completed epoch 14, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457835.5\n",
      "epoch loss is 1457836.3312637638\n",
      "completed epoch 15, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457808.75\n",
      "epoch loss is 1457810.4268187485\n",
      "completed epoch 16, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457782.0\n",
      "epoch loss is 1457784.4839269312\n",
      "completed epoch 17, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457755.125\n",
      "epoch loss is 1457756.5765245853\n",
      "completed epoch 18, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457728.625\n",
      "epoch loss is 1457729.5516046165\n",
      "completed epoch 19, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457701.875\n",
      "epoch loss is 1457703.026472911\n",
      "completed epoch 20, stepping optimiser\n",
      "Epoch: 21/300, loss: 30617374.0000\n",
      "task is 0\n",
      "calculated KL divergence 1457675.25\n",
      "epoch loss is 1457676.0069161437\n",
      "completed epoch 21, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457648.625\n",
      "epoch loss is 1457649.9134823084\n",
      "completed epoch 22, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457622.0\n",
      "epoch loss is 1457623.1773782077\n",
      "completed epoch 23, stepping optimiser\n",
      "task is 0\n",
      "calculated KL divergence 1457595.375\n",
      "epoch loss is 1457595.962233811\n",
      "completed epoch 24, stepping optimiser\n",
      "task is 0\n"
     ]
    }
   ],
   "source": [
    "#main training sequence\n",
    "\n",
    "bn = bayesian_network()\n",
    "data = generate_task_data(0)\n",
    "testdata = data[2]\n",
    "task_scores = [[],[],[],[],[]] \n",
    "bn.train_bayes_model(x = data[0],y =  data[1], lr = float(0.0001),n_epochs=300,task=0)\n",
    "testlabels = torch.tensor(data[3]).argmax(dim =1)\n",
    "testresult = bn.forward(torch.tensor(testdata).transpose(0,1),300,0)\n",
    "testresult = testresult.argmax(dim = 1)\n",
    "score = 0\n",
    "for k in range(len(testresult)):         ##\n",
    "    if testresult[i] == testlabels[i]:\n",
    "        score += 1\n",
    "task_scores[0].append(score/len(testresult))    #should just be able to repeat training here for other tasks \n",
    "create_new_prior_from_posterior() \n",
    "add_new_task_params()\n",
    "data = generate_task_data(2)\n",
    "testdata = data[2]\n",
    "bn.train_bayes_model(x = data[0],y =  data[1], lr = float(0.001),n_epochs=10,task=1)\n",
    "testlabels = torch.tensor(data[3]).argmax(dim =1)\n",
    "testresult = bn.forward(torch.tensor(testdata).transpose(0,1),300,0)\n",
    "testresult = testresult.argmax(dim = 1)\n",
    "score = 0\n",
    "for k in range(len(testresult)):\n",
    "    if testresult[i] == testlabels[i]:\n",
    "        score += 1\n",
    "task_scores[1].append(score/len(testresult))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
